{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3QTVQ-MeLv7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf2if_fGDaWc"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jrmj83afDJrv"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJEzDG6DK2Q"
      },
      "source": [
        "# Train a custom object detection model with TensorFlow Lite Model Maker\n",
        "\n",
        "In this colab notebook, you'll learn how to use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker) to train a custom object detection model to detect Android figurines and how to put the model on a Raspberry Pi.\n",
        "\n",
        "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYjtwRZGBOI"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "### Install the required packages\n",
        "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35BJmtVpAP_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cee983-ed29-4536-ace9-0f5e26182b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.3/577.3 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.6/128.6 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ86DdtD317"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4QQTXHHATDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b24d548-c030-4c9d-8c38-45cc8e5ed77f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6aQvXsD78P"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n",
        "We start with downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run only ONCE. Alternatively, change link to other Github repo or weblink.\n",
        "!wget https://github.com/Talonee/Portal/raw/main/pi/grass.zip\n",
        "!unzip -q grass.zip\n"
      ],
      "metadata": {
        "id": "5bU4AO7WixOt",
        "outputId": "830f93d1-5211-43f0-c15f-1f923579003f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-25 01:22:13--  https://github.com/Talonee/Portal/raw/main/pi/grass.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Talonee/Portal/main/pi/grass.zip [following]\n",
            "--2023-02-25 01:22:13--  https://raw.githubusercontent.com/Talonee/Portal/main/pi/grass.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59741808 (57M) [application/zip]\n",
            "Saving to: ‘grass.zip’\n",
            "\n",
            "grass.zip           100%[===================>]  56.97M   290MB/s    in 0.2s    \n",
            "\n",
            "2023-02-25 01:22:13 (290 MB/s) - ‘grass.zip’ saved [59741808/59741808]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Train the object detection model\n",
        "\n",
        "### Step 1: Load the dataset\n",
        "\n",
        "* Images in `train_data` is used to train the custom object detection model.\n",
        "* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to convert images\n",
        "from PIL import Image\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "\n",
        "dir = \"train/\"\n",
        "print(listdir(dir))\n",
        "\n",
        "for f in listdir(dir):\n",
        "    if (isfile(f\"{dir}{f}\") and re.search(\"jpg$\", f)):\n",
        "        # importing the image \n",
        "        im = Image.open(f\"{dir}{f}\")\n",
        "        if not im.mode == 'RGB':\n",
        "            print(f\"Ayo what the fuck{dir}{f}\")\n",
        "            # re-convert & save to jpg, wget downloads from google isn't recognized as JPEGs\n",
        "            im.convert('RGB').save(f\"{dir}{f}\",\"JPEG\")       "
      ],
      "metadata": {
        "id": "-j6TK9G3kujF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1995c2c-c50e-461e-f6da-0a000f5d755f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['13.jpg', '18.jpg', '93.jpg', '34.jpg', '78.xml', '32.jpg', '49.xml', '74.jpg', '76.xml', '50.jpg', '59.xml', '96.jpg', '9.xml', '82.xml', '99.xml', '22.jpg', '66.xml', '80.jpg', '99.jpg', '62.xml', '67.xml', '64.xml', '101.jpg', '73.jpg', '26.jpg', '35.xml', '79.jpg', '102.xml', '84.jpg', '101.xml', '14.xml', '94.xml', '80.xml', '31.xml', '78.jpg', '70.xml', '30.jpg', '94.jpg', '10.jpg', '26.xml', '70.jpg', '49.jpg', '7.jpg', '85.jpg', '7.xml', '100.jpg', '28.jpg', '73.xml', '69.xml', '29.xml', '56.jpg', '16.jpg', '9.jpg', '76.jpg', '34.xml', '21.jpg', '75.jpg', '86.xml', '67.jpg', '12.xml', '79.xml', '14.jpg', '69.jpg', '90.jpg', '63.xml', '32.xml', '64.jpg', '82.jpg', '83.xml', '11.xml', '62.jpg', '13.xml', '12.jpg', '47.xml', '16.xml', '50.xml', '29.jpg', '28.xml', '56.xml', '31.jpg', '5.xml', '3.xml', '66.jpg', '4.jpg', '68.xml', '19.jpg', '63.jpg', '35.jpg', '83.jpg', '58.jpg', '21.xml', '4.xml', '19.xml', '68.jpg', '8.jpg', '60.xml', '5.jpg', '100.xml', '86.jpg', '102.jpg', '93.xml', '85.xml', '60.jpg', '47.jpg', '58.xml', '59.jpg', '95.jpg', '22.xml', '74.xml', '3.jpg', '30.xml', '11.jpg', '8.xml', '84.xml', '90.xml', '88.xml', '10.xml', '18.xml', '95.xml', '88.jpg', '96.xml', '75.xml']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'train',\n",
        "    'train',\n",
        "    ['grass']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'validate',\n",
        "    'validate',\n",
        "    ['grass']\n",
        ")\n",
        "\n",
        "# See how many images are available in each set: train, validate.\n",
        "print(f\"Train: {len(train_data)} images\\nValidate: {len(val_data)} images\")"
      ],
      "metadata": {
        "id": "JCGhcyp7i3Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23762f82-bd99-457b-9e59-6c0b3b3fe0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 61 images\n",
            "Validate: 18 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Step 2: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "model_id = 0\n",
        "spec = model_spec.get(f'efficientdet_lite{model_id}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "### Step 3: Train the TensorFlow model with the training data.\n",
        "\n",
        "* Set `epochs = 20`, which means it will go through the training dataset 20 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MClfpsJAfda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa3cb79-42e9-4bfd-9636-582e34ca2c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "15/15 [==============================] - 59s 972ms/step - det_loss: 1.7360 - cls_loss: 1.1091 - box_loss: 0.0125 - reg_l2_loss: 0.0630 - loss: 1.7990 - learning_rate: 0.0065 - gradient_norm: 2.3434 - val_det_loss: 1.5882 - val_cls_loss: 1.0756 - val_box_loss: 0.0103 - val_reg_l2_loss: 0.0630 - val_loss: 1.6512\n",
            "Epoch 2/20\n",
            "15/15 [==============================] - 12s 826ms/step - det_loss: 1.5982 - cls_loss: 1.0174 - box_loss: 0.0116 - reg_l2_loss: 0.0630 - loss: 1.6612 - learning_rate: 0.0049 - gradient_norm: 2.8597 - val_det_loss: 1.4225 - val_cls_loss: 0.9681 - val_box_loss: 0.0091 - val_reg_l2_loss: 0.0630 - val_loss: 1.4855\n",
            "Epoch 3/20\n",
            "15/15 [==============================] - 11s 766ms/step - det_loss: 1.4012 - cls_loss: 0.8946 - box_loss: 0.0101 - reg_l2_loss: 0.0630 - loss: 1.4642 - learning_rate: 0.0048 - gradient_norm: 3.7608 - val_det_loss: 1.1385 - val_cls_loss: 0.7457 - val_box_loss: 0.0079 - val_reg_l2_loss: 0.0630 - val_loss: 1.2016\n",
            "Epoch 4/20\n",
            "15/15 [==============================] - 10s 708ms/step - det_loss: 1.1737 - cls_loss: 0.6971 - box_loss: 0.0095 - reg_l2_loss: 0.0630 - loss: 1.2367 - learning_rate: 0.0046 - gradient_norm: 4.4275 - val_det_loss: 0.7651 - val_cls_loss: 0.4118 - val_box_loss: 0.0071 - val_reg_l2_loss: 0.0630 - val_loss: 0.8282\n",
            "Epoch 5/20\n",
            "15/15 [==============================] - 18s 1s/step - det_loss: 1.0669 - cls_loss: 0.6056 - box_loss: 0.0092 - reg_l2_loss: 0.0630 - loss: 1.1299 - learning_rate: 0.0043 - gradient_norm: 4.4584 - val_det_loss: 1.0458 - val_cls_loss: 0.6967 - val_box_loss: 0.0070 - val_reg_l2_loss: 0.0631 - val_loss: 1.1088\n",
            "Epoch 6/20\n",
            "15/15 [==============================] - 13s 893ms/step - det_loss: 0.8961 - cls_loss: 0.4984 - box_loss: 0.0080 - reg_l2_loss: 0.0631 - loss: 0.9591 - learning_rate: 0.0040 - gradient_norm: 3.9659 - val_det_loss: 1.0715 - val_cls_loss: 0.7466 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.0631 - val_loss: 1.1346\n",
            "Epoch 7/20\n",
            "15/15 [==============================] - 10s 695ms/step - det_loss: 0.8186 - cls_loss: 0.4420 - box_loss: 0.0075 - reg_l2_loss: 0.0631 - loss: 0.8816 - learning_rate: 0.0037 - gradient_norm: 4.0384 - val_det_loss: 1.1229 - val_cls_loss: 0.8062 - val_box_loss: 0.0063 - val_reg_l2_loss: 0.0631 - val_loss: 1.1860\n",
            "Epoch 8/20\n",
            "15/15 [==============================] - 12s 803ms/step - det_loss: 0.8486 - cls_loss: 0.4524 - box_loss: 0.0079 - reg_l2_loss: 0.0631 - loss: 0.9117 - learning_rate: 0.0033 - gradient_norm: 5.1814 - val_det_loss: 0.8241 - val_cls_loss: 0.5261 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0631 - val_loss: 0.8872\n",
            "Epoch 9/20\n",
            "15/15 [==============================] - 11s 778ms/step - det_loss: 0.7422 - cls_loss: 0.4127 - box_loss: 0.0066 - reg_l2_loss: 0.0631 - loss: 0.8053 - learning_rate: 0.0029 - gradient_norm: 4.1661 - val_det_loss: 0.6471 - val_cls_loss: 0.3615 - val_box_loss: 0.0057 - val_reg_l2_loss: 0.0631 - val_loss: 0.7102\n",
            "Epoch 10/20\n",
            "15/15 [==============================] - 15s 1s/step - det_loss: 0.6603 - cls_loss: 0.3533 - box_loss: 0.0061 - reg_l2_loss: 0.0631 - loss: 0.7234 - learning_rate: 0.0025 - gradient_norm: 4.4091 - val_det_loss: 0.5307 - val_cls_loss: 0.2973 - val_box_loss: 0.0047 - val_reg_l2_loss: 0.0631 - val_loss: 0.5938\n",
            "Epoch 11/20\n",
            "15/15 [==============================] - 9s 617ms/step - det_loss: 0.6923 - cls_loss: 0.3727 - box_loss: 0.0064 - reg_l2_loss: 0.0631 - loss: 0.7554 - learning_rate: 0.0021 - gradient_norm: 4.4956 - val_det_loss: 0.4509 - val_cls_loss: 0.2465 - val_box_loss: 0.0041 - val_reg_l2_loss: 0.0631 - val_loss: 0.5140\n",
            "Epoch 12/20\n",
            "15/15 [==============================] - 12s 851ms/step - det_loss: 0.6582 - cls_loss: 0.3688 - box_loss: 0.0058 - reg_l2_loss: 0.0631 - loss: 0.7213 - learning_rate: 0.0017 - gradient_norm: 5.8719 - val_det_loss: 0.4083 - val_cls_loss: 0.2287 - val_box_loss: 0.0036 - val_reg_l2_loss: 0.0631 - val_loss: 0.4714\n",
            "Epoch 13/20\n",
            "15/15 [==============================] - 13s 912ms/step - det_loss: 0.6273 - cls_loss: 0.3326 - box_loss: 0.0059 - reg_l2_loss: 0.0631 - loss: 0.6904 - learning_rate: 0.0013 - gradient_norm: 4.9914 - val_det_loss: 0.3608 - val_cls_loss: 0.2035 - val_box_loss: 0.0031 - val_reg_l2_loss: 0.0631 - val_loss: 0.4239\n",
            "Epoch 14/20\n",
            "15/15 [==============================] - 11s 749ms/step - det_loss: 0.6169 - cls_loss: 0.3347 - box_loss: 0.0056 - reg_l2_loss: 0.0631 - loss: 0.6800 - learning_rate: 9.6772e-04 - gradient_norm: 4.7291 - val_det_loss: 0.3791 - val_cls_loss: 0.2244 - val_box_loss: 0.0031 - val_reg_l2_loss: 0.0631 - val_loss: 0.4422\n",
            "Epoch 15/20\n",
            "15/15 [==============================] - 16s 1s/step - det_loss: 0.6002 - cls_loss: 0.3166 - box_loss: 0.0057 - reg_l2_loss: 0.0631 - loss: 0.6634 - learning_rate: 6.6413e-04 - gradient_norm: 5.5079 - val_det_loss: 0.3800 - val_cls_loss: 0.2311 - val_box_loss: 0.0030 - val_reg_l2_loss: 0.0631 - val_loss: 0.4432\n",
            "Epoch 16/20\n",
            "15/15 [==============================] - 11s 740ms/step - det_loss: 0.5862 - cls_loss: 0.3178 - box_loss: 0.0054 - reg_l2_loss: 0.0631 - loss: 0.6493 - learning_rate: 4.1061e-04 - gradient_norm: 4.1700 - val_det_loss: 0.3760 - val_cls_loss: 0.2297 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0631 - val_loss: 0.4391\n",
            "Epoch 17/20\n",
            "15/15 [==============================] - 10s 660ms/step - det_loss: 0.5435 - cls_loss: 0.2980 - box_loss: 0.0049 - reg_l2_loss: 0.0631 - loss: 0.6066 - learning_rate: 2.1409e-04 - gradient_norm: 3.6105 - val_det_loss: 0.3858 - val_cls_loss: 0.2410 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0631 - val_loss: 0.4490\n",
            "Epoch 18/20\n",
            "15/15 [==============================] - 12s 815ms/step - det_loss: 0.5684 - cls_loss: 0.3101 - box_loss: 0.0052 - reg_l2_loss: 0.0631 - loss: 0.6315 - learning_rate: 7.9920e-05 - gradient_norm: 4.0847 - val_det_loss: 0.3831 - val_cls_loss: 0.2416 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0631 - val_loss: 0.4462\n",
            "Epoch 19/20\n",
            "15/15 [==============================] - 11s 753ms/step - det_loss: 0.5755 - cls_loss: 0.3208 - box_loss: 0.0051 - reg_l2_loss: 0.0631 - loss: 0.6387 - learning_rate: 1.1764e-05 - gradient_norm: 4.1917 - val_det_loss: 0.3663 - val_cls_loss: 0.2300 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.4294\n",
            "Epoch 20/20\n",
            "15/15 [==============================] - 14s 1s/step - det_loss: 0.4889 - cls_loss: 0.2736 - box_loss: 0.0043 - reg_l2_loss: 0.0631 - loss: 0.5521 - learning_rate: 1.1480e-05 - gradient_norm: 3.6614 - val_det_loss: 0.3701 - val_cls_loss: 0.2327 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0631 - val_loss: 0.4332\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Step 4. Evaluate the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUqEpcYwAg8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be39638-1879-41c0-aea9-e65304fa870a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r1/1 [==============================] - 9s 9s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.5904379,\n",
              " 'AP50': 0.84582746,\n",
              " 'AP75': 0.7335831,\n",
              " 'APs': -1.0,\n",
              " 'APm': -1.0,\n",
              " 'APl': 0.59048575,\n",
              " 'ARmax1': 0.6,\n",
              " 'ARmax10': 0.65555555,\n",
              " 'ARmax100': 0.6722222,\n",
              " 'ARs': -1.0,\n",
              " 'ARm': -1.0,\n",
              " 'ARl': 0.6722222,\n",
              " 'AP_/grass': 0.5904379}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Step 5: Export as a TensorFlow Lite model.\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3eFxoBAiqE"
      },
      "outputs": [],
      "source": [
        "tflite_name=f'grass{model_id}.tflite'\n",
        "model.export(export_dir='.', tflite_filename=tflite_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Step 6:  Evaluate the TensorFlow Lite model.\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbl8z9_wBPlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e858e0d9-bde8-4908-8693-7b9ee1ade81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 57s 3s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.5651574,\n",
              " 'AP50': 0.8394497,\n",
              " 'AP75': 0.7116337,\n",
              " 'APs': -1.0,\n",
              " 'APm': -1.0,\n",
              " 'APl': 0.5651574,\n",
              " 'ARmax1': 0.59444445,\n",
              " 'ARmax10': 0.6166667,\n",
              " 'ARmax100': 0.6166667,\n",
              " 'ARs': -1.0,\n",
              " 'ARm': -1.0,\n",
              " 'ARl': 0.6166667,\n",
              " 'AP_/grass': 0.5651574}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# Highest performance: 67%\n",
        "model.evaluate_tflite(tflite_name, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7zgUkdOUUnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "161da06a-5884-4e9a-b1ee-2ad8290af87c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c418c6c2-92f9-43c6-b01b-a0419ce18387\", \"grass0.tflite\", 4444716)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download the TFLite model to your local computer.\n",
        "from google.colab import files\n",
        "files.download(tflite_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}